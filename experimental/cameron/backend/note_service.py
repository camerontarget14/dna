import os
import random
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()


class LLMSummaryRequest(BaseModel):
    text: str


# --- LLM IMPLEMENTATION CODE ---

import os
import requests
from openai import OpenAI
import anthropic
import google.generativeai as genai
import re
from dotenv import load_dotenv

# === CONFIGURABLE PARAMETERS ===
TEMPERATURE = 0.1

SYSTEM_PROMPT = """
You are a helpful assistant that reviews audio transcripts of meetings and recreates short and precise abbreviated conversations between people from them.
The meeting is about discussion of creative work submissions by artists, who are working on parts of a movie (called 'shots').
The intent of the meeting is to review those submissions one by one and provide clear feedback and suggestions to the artist.
The abbreviated conversations generated by you are meant for the artists to quickly read to get the gist of exactly what was said by who and understand
the next steps, if any.
"""

USER_PROMPT_TEMPLATE = """
The following is a conversation about a shot.

Create notes on specific creative decisions and any actionable tasks for each of them. Be as concisie and direct as possible in the notes.
You can include the speaker initials in the notes to identify the person who made the point. Highlight any important decisions reached,
such as a shot being approved (finalled) by the creative lead.

Just generate short/consise notes from the given conversation, without any header or footer text such as subject line or follow up questions.

Following is the conversation:
{conversation}

"""

DEFAULT_MODELS = {
    "openai": "gpt-4o",
    "claude": "claude-3-sonnet-20240229",
    "ollama": "llama3.2",
    "gemini": "gemini-2.5-flash-preview-05-20",
}


def summarize_openai(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=TEMPERATURE,
    )
    return response.choices[0].message.content


def summarize_claude(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=TEMPERATURE,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
    )
    return response.content[0].text


def summarize_ollama(conversation, model, client):
    prompt = (
        SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE.format(conversation=conversation)
    )
    response = client.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False},
    )
    return response.json()["response"]


def summarize_gemini(conversation, model, client):
    full_prompt = (
        f"{SYSTEM_PROMPT}\n\n{USER_PROMPT_TEMPLATE.format(conversation=conversation)}"
    )
    response = client.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            max_output_tokens=1024,
            temperature=TEMPERATURE,
        ),
    )
    if not response.candidates:
        raise Exception("No response candidates returned from Gemini")
    candidate = response.candidates[0]
    if candidate.finish_reason == 2:
        raise Exception("Response blocked by Gemini safety filters")
    elif candidate.finish_reason == 3:
        raise Exception("Response blocked due to recitation concerns")
    elif candidate.finish_reason == 4:
        raise Exception("Response blocked for other reasons")
    if not candidate.content or not candidate.content.parts:
        raise Exception("No content parts in response")
    return candidate.content.parts[0].text


def create_llm_client(provider, api_key=None, model=None):
    provider = provider.lower()
    if provider == "openai":
        if not api_key:
            raise ValueError("OpenAI requires an api_key.")
        return OpenAI(api_key=api_key)
    elif provider == "claude":
        if not api_key:
            raise ValueError("Anthropic Claude requires an api_key.")
        return anthropic.Anthropic(api_key=api_key)
    elif provider == "ollama":
        return requests.Session()
    elif provider == "gemini":
        if not api_key:
            raise ValueError("Gemini requires an api_key.")
        if not model:
            raise ValueError("Gemini requires a model name.")
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model)
    else:
        raise ValueError(f"Unsupported provider: {provider}")


# --- LLM client cache ---
# Initialize all available LLM clients
openai_api_key = os.getenv("OPENAI_API_KEY")
openai_model = DEFAULT_MODELS["openai"]
openai_client = None
if openai_api_key:
    try:
        openai_client = create_llm_client(
            "openai", api_key=openai_api_key, model=openai_model
        )
        print("OpenAI client initialized successfully")
    except Exception as e:
        print(f"Error initializing OpenAI client: {e}")

gemini_api_key = os.getenv("GEMINI_API_KEY")
gemini_model = DEFAULT_MODELS["gemini"]
gemini_client = None
if gemini_api_key and gemini_api_key != "your-gemini-api-key":
    try:
        gemini_client = create_llm_client(
            "gemini", api_key=gemini_api_key, model=gemini_model
        )
        print("Gemini client initialized successfully")
    except Exception as e:
        print(f"Error initializing Gemini client: {e}")

anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
anthropic_model = DEFAULT_MODELS["claude"]
anthropic_client = None
if anthropic_api_key and anthropic_api_key != "your-anthropic-api-key":
    try:
        anthropic_client = create_llm_client(
            "claude", api_key=anthropic_api_key, model=anthropic_model
        )
        print("Anthropic client initialized successfully")
    except Exception as e:
        print(f"Error initializing Anthropic client: {e}")

DISABLE_LLM = os.getenv("DISABLE_LLM", "true").lower() in ("1", "true", "yes")


@router.post("/llm-summary")
async def llm_summary(data: LLMSummaryRequest):
    """
    Generate a summary using LLM for the given text.
    Tries providers in order: OpenAI -> Gemini -> Anthropic
    """
    if DISABLE_LLM:
        # Return a random summary for testing
        random_summaries = [
            "The team discussed lighting and animation improvements.",
            "Minor tweaks needed for character animation; background approved.",
            "Action items: soften shadows, adjust highlight gain, improve hand motion.",
            "Most notes addressed; only a few minor issues remain.",
            "Ready for final review after next round of changes.",
            "Feedback: color grade is close, but highlights too hot.",
            "Artist to be notified about animation and lighting feedback.",
            "Overall progress is good; next steps communicated to the team.",
        ]
        return {"summary": random.choice(random_summaries)}

    # Try OpenAI first (current preference)
    if openai_client:
        try:
            print(f"Using OpenAI ({openai_model}) for summary")
            summary = summarize_openai(data.text, openai_model, openai_client)
            return {"summary": summary}
        except Exception as e:
            print(f"Error with OpenAI: {e}")

    # Fallback to Gemini
    if gemini_client:
        try:
            print(f"Using Gemini ({gemini_model}) for summary")
            summary = summarize_gemini(data.text, gemini_model, gemini_client)
            return {"summary": summary}
        except Exception as e:
            print(f"Error with Gemini: {e}")

    # Fallback to Anthropic
    if anthropic_client:
        try:
            print(f"Using Anthropic ({anthropic_model}) for summary")
            summary = summarize_claude(data.text, anthropic_model, anthropic_client)
            return {"summary": summary}
        except Exception as e:
            print(f"Error with Anthropic: {e}")

    # No providers available
    raise HTTPException(
        status_code=500,
        detail="No LLM providers available. Please configure at least one API key (OPENAI_API_KEY, GEMINI_API_KEY, or ANTHROPIC_API_KEY) in .env file.",
    )
