import os
import random

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()


class LLMSummaryRequest(BaseModel):
    text: str


class GenerateAINotesRequest(BaseModel):
    """Request to generate AI notes from transcript"""

    version_id: str
    transcript: str = None  # If not provided, uses version's existing transcript


# --- LLM IMPLEMENTATION CODE ---

import os
import re

import anthropic
import google.generativeai as genai
import requests
from dotenv import load_dotenv
from openai import OpenAI

# === CONFIGURABLE PARAMETERS ===
TEMPERATURE = 0.1

SYSTEM_PROMPT = """
You are a helpful assistant that reviews audio transcripts of meetings and recreates short and precise abbreviated conversations between people from them.
The meeting is about discussion of creative work submissions by artists, who are working on parts of a movie (called 'shots').
The intent of the meeting is to review those submissions one by one and provide clear feedback and suggestions to the artist.
The abbreviated conversations generated by you are meant for the artists to quickly read to get the gist of exactly what was said by who and understand
the next steps, if any.
"""

USER_PROMPT_TEMPLATE = """
The following is a conversation about a shot.

Create notes on specific creative decisions and any actionable tasks for each of them. Be as concisie and direct as possible in the notes.
You can include the speaker initials in the notes to identify the person who made the point. Highlight any important decisions reached,
such as a shot being approved (finalled) by the creative lead.

Just generate short/consise notes from the given conversation, without any header or footer text such as subject line or follow up questions.

Following is the conversation:
{conversation}

"""

DEFAULT_MODELS = {
    "openai": "gpt-4o",
    "claude": "claude-3-sonnet-20240229",
    "ollama": "llama3.2",
    "gemini": "gemini-2.5-flash-preview-05-20",
}


def summarize_openai(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=TEMPERATURE,
    )
    return response.choices[0].message.content


def summarize_claude(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=TEMPERATURE,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
    )
    return response.content[0].text


def summarize_ollama(conversation, model, client):
    prompt = (
        SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE.format(conversation=conversation)
    )
    response = client.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False},
    )
    return response.json()["response"]


def summarize_gemini(conversation, model, client):
    full_prompt = (
        f"{SYSTEM_PROMPT}\n\n{USER_PROMPT_TEMPLATE.format(conversation=conversation)}"
    )
    response = client.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            max_output_tokens=1024,
            temperature=TEMPERATURE,
        ),
    )
    if not response.candidates:
        raise Exception("No response candidates returned from Gemini")
    candidate = response.candidates[0]
    if candidate.finish_reason == 2:
        raise Exception("Response blocked by Gemini safety filters")
    elif candidate.finish_reason == 3:
        raise Exception("Response blocked due to recitation concerns")
    elif candidate.finish_reason == 4:
        raise Exception("Response blocked for other reasons")
    if not candidate.content or not candidate.content.parts:
        raise Exception("No content parts in response")
    return candidate.content.parts[0].text


def create_llm_client(provider, api_key=None, model=None):
    provider = provider.lower()
    if provider == "openai":
        if not api_key:
            raise ValueError("OpenAI requires an api_key.")
        return OpenAI(api_key=api_key)
    elif provider == "claude":
        if not api_key:
            raise ValueError("Anthropic Claude requires an api_key.")
        return anthropic.Anthropic(api_key=api_key)
    elif provider == "ollama":
        return requests.Session()
    elif provider == "gemini":
        if not api_key:
            raise ValueError("Gemini requires an api_key.")
        if not model:
            raise ValueError("Gemini requires a model name.")
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model)
    else:
        raise ValueError(f"Unsupported provider: {provider}")


# --- LLM client cache ---
# Initialize all available LLM clients
openai_api_key = os.getenv("OPENAI_API_KEY")
openai_model = DEFAULT_MODELS["openai"]
openai_client = None
if openai_api_key:
    try:
        openai_client = create_llm_client(
            "openai", api_key=openai_api_key, model=openai_model
        )
        print("OpenAI client initialized successfully")
    except Exception as e:
        print(f"Error initializing OpenAI client: {e}")

gemini_api_key = os.getenv("GEMINI_API_KEY")
gemini_model = DEFAULT_MODELS["gemini"]
gemini_client = None
if gemini_api_key and gemini_api_key != "your-gemini-api-key":
    try:
        gemini_client = create_llm_client(
            "gemini", api_key=gemini_api_key, model=gemini_model
        )
        print("Gemini client initialized successfully")
    except Exception as e:
        print(f"Error initializing Gemini client: {e}")

anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
anthropic_model = DEFAULT_MODELS["claude"]
anthropic_client = None
if anthropic_api_key and anthropic_api_key != "your-anthropic-api-key":
    try:
        anthropic_client = create_llm_client(
            "claude", api_key=anthropic_api_key, model=anthropic_model
        )
        print("Anthropic client initialized successfully")
    except Exception as e:
        print(f"Error initializing Anthropic client: {e}")

DISABLE_LLM = os.getenv("DISABLE_LLM", "true").lower() in ("1", "true", "yes")


@router.post("/llm-summary")
async def llm_summary(data: LLMSummaryRequest):
    """
    Generate a summary using LLM for the given text.
    Tries providers in order: OpenAI -> Gemini -> Anthropic
    """
    summary = generate_summary(data.text)
    return {"summary": summary}


def generate_summary(text: str) -> str:
    """
    Internal function to generate a summary using LLM for the given text.
    This can be imported directly by other services to avoid HTTP overhead.
    Tries providers in order: OpenAI -> Gemini -> Anthropic
    """
    if DISABLE_LLM:
        # Return a random summary for testing
        random_summaries = [
            "The team discussed lighting and animation improvements.",
            "Minor tweaks needed for character animation; background approved.",
            "Action items: soften shadows, adjust highlight gain, improve hand motion.",
            "Most notes addressed; only a few minor issues remain.",
            "Ready for final review after next round of changes.",
            "Feedback: color grade is close, but highlights too hot.",
            "Artist to be notified about animation and lighting feedback.",
            "Overall progress is good; next steps communicated to the team.",
        ]
        return random.choice(random_summaries)

    # Try OpenAI first (current preference)
    if openai_client:
        try:
            print(f"Using OpenAI ({openai_model}) for summary")
            summary = summarize_openai(text, openai_model, openai_client)
            return summary
        except Exception as e:
            print(f"Error with OpenAI: {e}")

    # Fallback to Gemini
    if gemini_client:
        try:
            print(f"Using Gemini ({gemini_model}) for summary")
            summary = summarize_gemini(text, gemini_model, gemini_client)
            return summary
        except Exception as e:
            print(f"Error with Gemini: {e}")

    # Fallback to Anthropic
    if anthropic_client:
        try:
            print(f"Using Anthropic ({anthropic_model}) for summary")
            summary = summarize_claude(text, anthropic_model, anthropic_client)
            return summary
        except Exception as e:
            print(f"Error with Anthropic: {e}")

    # No providers available
    raise HTTPException(
        status_code=500,
        detail="No LLM providers available. Please configure at least one API key (OPENAI_API_KEY, GEMINI_API_KEY, or ANTHROPIC_API_KEY) in .env file.",
    )


@router.post("/generate-ai-notes")
async def generate_ai_notes_endpoint(request: GenerateAINotesRequest):
    """
    Generate AI notes from transcript for a version.
    This endpoint requires access to version storage from version_service.
    """
    # Import here to avoid circular dependency
    from version_service import _versions

    version_id = request.version_id

    if version_id not in _versions:
        raise HTTPException(status_code=404, detail=f"Version '{version_id}' not found")

    version = _versions[version_id]

    # Use provided transcript or version's existing transcript
    transcript = request.transcript if request.transcript else version.transcript

    if not transcript:
        raise HTTPException(
            status_code=400, detail="No transcript available for AI note generation"
        )

    # Generate AI notes using the internal function
    ai_notes = generate_summary(transcript)

    # Store AI notes in version
    version.ai_notes = ai_notes

    return {"status": "success", "version": version.model_dump()}
